<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="WANDR: Intention-guided Human Motion Generation">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>WANDR: Intention-guided Human Motion Generation</title>


  <link href="./static/css/google_sans_noto_sans_castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <!-- <link rel="stylesheet" href="./static/css/fontawesome.all.min.css"> -->
  <link rel="stylesheet"
        href="./static/css/academicons.min.css">
        <!-- href="./static/css/academicons.min.css"> -->
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <!-- <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> -->
  <script id="MathJax-script" async src="./static/js/tex-mml-chtml.js"></script>

  <script src="./static/js/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">WANDR: Intention-guided Human Motion Generation</h1>
          <h3 class="title is-3 publication-title">CVPR 2024</h3>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ps.is.mpg.de/person/mdiomataris" target="_blank">Markos Diomataris</a><sup>1</sup><sup> 2</sup>,</span>
              <!-- <a href="https://markos-diomataris.github.io/">Markos Diomataris</a><sup>1</sup><sup> 2</sup>,</span> -->
            <span class="author-block">
              <a href="https://is.mpg.de/~nathanasiou" target="_blank">Nikos Athanasiou</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://otaheri.github.io/" target="_blank">Omid Taheri</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://xiwang1212.github.io/homepage/" target="_blank">Xi Wang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://ait.ethz.ch/people/hilliges" target="_blank">Otmar Hilliges</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://ps.is.mpg.de/person/black" target="_blank">Michael J. Black</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Max Planck Institute for Intelligent Systems, Tübingen, Germany,</span><br>
            <span class="author-block"><sup>2</sup>ETH Zürich, Switzerland</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="index.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <img src="./static/css/pdf.svg" width="15">
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="index.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <img src="./static/css/arxiv.svg" width="15">
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/9szizM-XUCg?si=B836zQoWTI4I9s61" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <img src="./static/css/video.svg" width="15">
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/markos-diomataris/wandr"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <img src="./static/css/github.svg" width="15">
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section>
<!-- 
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered text-font">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          Synthesizing natural human motions that enable a 3D human avatar to walk and reach for arbitrary goals in 3D space remains an unsolved problem with many applications. Existing methods (data-driven or using reinforcement learning) are limited in terms of generalization and motion naturalness.
          A primary obstacle is the scarcity of training data that combines locomotion with goal reaching.
          </p>
          <p>
          To address this, we introduce WANDR, a data-driven model that takes an avatar’s initial pose and a goal’s 3D position and generates natural human motions that place the end effector (wrist) on the goal location. To solve this, we introduce novel intention features that drive rich goal-oriented movement.
          </p>
          <p>
           Intention guides the agent to the goal, and interactively adapts the generation to novel situations without needing to define sub-goals or the entire motion path. Crucially, intention allows training on datasets that have goal-oriented motions as well as those that do not. WANDR is a conditional Variational Auto-Encoder (c-VAE), which we train using the AMASS and CIRCLE datasets. We evaluate our method extensively and demonstrate its ability to generate natural and long-term motions that reach 3D goals and generalize to unseen goal locations.
          </p>
        </div>
      </div>
    </div>
  </div>
  <br>
  <br>
</section> -->


<section class="hero teaser text-font">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">What is WANDR?</h2>
        <div class="content has-text-justified">
          <p>
            <!-- write basic discription of wandr and have wandr in bold -->
            <span class="wandr">WANDR</span> is a conditional Variational AutoEncoder (c-VAE) that generates realistic motion of human avatars that
            navigate towards an arbitrary goal location and reach for it.
            <b>Input</b> to our method is the initial pose of the avatar, the goal location, and the desired motion duration.
            <b>Output</b> is a sequence of poses that guide the avatar
             from the initial pose to the goal location and place the wrist on it. 
            
          </p>
        </div>
      </div>
    </div>
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/branching_motion.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered small_font">
       Starting from the same state, WANDR generates diverse motions to reach different goal locations all around the human. 
      </h2>
    </div>
  </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">How is WANDR unique?</h2>
        <div class="content has-text-justified">
          <p>
            <span class="wandr">WANDR</span> is the first human motion generation model that is driven by an <b>active feedback loop learned purely from data</b>,
             without any extra steps of reinforcement learning (RL).
              <div class="is-centered has-text-centered small-header">Active closed loop guidance through <b><span class="int">"intention"</span></b> features.</div>
               <span class="wandr">WANDR</span>  generates motion autoregressively (frame-by-frame). At each step, it predicts a state-delta that will progress the human to the next state.
                The prediction of the state-delta is conditioned on time- and goal-dependent features that we call "intention" (visualized as arrows in videos below). 
                These features are computed at every frame and act as a feedback loop that guides the motion generation to reach the goal. For more details on the intention, please refer to section 3.2 of the paper.
              <div class="is-centered has-text-centered small-header">Purely data-driven training.</div>
              Existing datasets that capure motion of humans reaching for goals, like <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Araujo_CIRCLE_Capture_in_Rich_Contextual_Environments_CVPR_2023_paper.pdf" target="_blank">CIRCLE</a>, are scarce have very small scale to enable generalization.
              This is why <a href="https://arxiv.org/pdf/2310.04582.pdf" target="_blank">RL</a> is a popular approach to learn similar tasks. However, RL comes with its own set of challenges such as sample complexity.
              Inspired by the paradigm of behavioral cloning we propose a purely data-driven approach where during training a future position of the avatar's hand is considered as the goal. By halucinating goals this way, we are able to combine
              both smaller datasets with goal annotations such as <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Araujo_CIRCLE_Capture_in_Rich_Contextual_Environments_CVPR_2023_paper.pdf" target="_blank">CIRCLE</a>, 
              as well as large scale like <a href="https://amass.is.tue.mpg.de/" target="_blank">AMASS</a> that have no goal labels but are essential to learning general navigational skills suck as walking, turning etc.
          </p>
        </div>
      </div>
    </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/motion1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/motion2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/motion3.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/motion4.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/motion5.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/motion6.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <h2 class="subtitle has-text-centered small_font">
        Examples of <span class="wandr">WANDR</span> generated motions with different initial poses, goals and durations. Notice how the <span class="int">"intention"</span> features (arrows) <b>actively guide</b> the avatar to reach the goal.
      </h2>
    </div>
  </div>
</section>


<br>
<br>
<section class="hero teaser text-font">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Method</h2>
      <div class="content has-text-justified">
        <p>
        Our method is based on a conditional Variational Auto-Encoder (c-VAE) that learns to model motion as a frame-by-frame generation process by auto-encoding the pose difference between two adjacent frames.
        The condition signal consists of the human’s current pose and dynamics along with the intention information.
        Intention is a function of both the current pose and the goal location and therefore actively guides the avatar during the motion generation in a closed loop manner.
        Through training, the c-VAE learns the distribution of potential subsequent poses conditioned on the current dynamic state of the human and its intention towards a specific goal.
        We train WANDR using two datasets: <a href="https://amass.is.tue.mpg.de/" target="_blank">AMASS</a>, which captures a wide range of motions including locomotion, and <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Araujo_CIRCLE_Capture_in_Rich_Contextual_Environments_CVPR_2023_paper.pdf" target="_blank">CIRCLE</a>, which captures reaching motions.
        During inference, intention features are calculated based on the goal and act as a feedback loop that guides the motion generation towards the goal.
        </p>
      </div>
    </div>
  </div>
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <div class="content has-text-justified">
          <img src="./images/method.png"
          class="interpolation-image small-width"
          alt="method"/>
          <!-- <figcaption>Caption text goes here</figcaption> -->
      </div>
    </div>
  </div>
  <br>
  <br>
</section>



<section class="hero teaser text-font">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Adapting to dynamic goals without training for it</h2>
        <div class="content has-text-justified">
          <p>
          Since <span class="wandr">WANDR</span> generates motion autoregressively, the intention features are updated at every frame. This allows the model to adapt to goals that move and change over time.
            Observe in the videos below how the intention features actively guide the avatar to orient its self towars the goal (orange arrow), get close to it (red arrow) and reach for it (blue arrow).
          </p>
        </div>
      </div>
    </div>
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/moving_goal1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/multiple_goals4.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/moving_goal2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/multiple_goals3.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/moving_goal5.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <h2 class="subtitle has-text-centered small_font">
        <span class="wandr">WANDR</span> generates motion autoregressively. This allows it to adapt to goals that move and change over time even though it has never been trained on scenarios with dynamic goals. 
      </h2>
    </div>
  </div>
</section>

<section class="section text-font">
  <div class="container is-max-desktop">
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/9szizM-XUCg?si=G7HDNmDWJRAAI0Ze" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
<!--  https://www.youtube.com/watch?v=9szizM-XUCg -->
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{diomataris2024wandr,
  title = {{WANDR}: Intention-guided Human Motion Generation},
  author = {Diomataris, Markos and Athanasiou, Nikos and Taheri, Omid and Wang, Xi and Hilliges, Otmar and Black, Michael J.},
  booktitle = {Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2024},
}</code></pre>
  </div>
</section>


    
        
        <footer class="footer">
            <div class="container">
                <div class="content has-text-centered">
                    <a class="icon-link"
                        href="">
                    <i class="fas fa-file-pdf"></i>
                    </a>
                    <a class="icon-link" href="https://github.com/markos-diomataris" class="external-link" disabled>
                    <i class="fab fa-github"></i>
                    </a>
                </div>
                <div class="columns is-centered">
                    <div class="column is-8">
                        <div class="content">
                            <!-- <p>
                                This website is licensed under a <a rel="license"
                                    href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                                Commons Attribution-ShareAlike 4.0 International License</a>.
                            </p> -->
                            <p>
                            This webpage is built with the template from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>. We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
                            </p>
							<p>
							<small>© 2024 Max-Planck-Gesellschaft <small class="text-muted ml-1 mr-1">-</small><a href="imprint.html">Imprint</a><small class="text-muted ml-1 mr-1">-</small><a href="privacy-policy.html">Privacy Policy</a><small class="text-muted ml-1 mr-1">-</small><a href="license.html">License</a></small>
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </footer>
    </body>
</html>
